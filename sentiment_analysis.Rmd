---
title: "sentiment"
author: "athemiiix"
date: "January 24, 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}

library(tidytext)

get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")

```

```{r}
library(janeaustenr)
library(dplyr)
library(stringr)

tidy_books <- austen_books() %>% 
  group_by(book) %>% 
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
                                                 ignore_case = TRUE)))) %>% 
  ungroup() %>% 
  unnest_tokens(word, text)
```

```{r}
nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

tidy_books %>% 
  filter(book == "Emma") %>% 
  inner_join(nrc_joy) %>% 
  count(word, sort = TRUE)

```

```{r}
library(tidyr)

jane_austen_sentiment <- tidy_books %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(book, index = linenumber %/% 80, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(sentiment = positive - negative)

library(ggplot2)

ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")

```


```{r}
pride_prejudice <- tidy_books %>% 
  filter(book == "Pride & Prejudice")

```

```{r}
afinn <- pride_prejudice %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = linenumber %/% 80) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")
```

```{r}
bing_and_nrc <- bind_rows(pride_prejudice %>% 
                            inner_join(get_sentiments("bing")) %>% 
                            mutate(method = "Bing et al."),
                          pride_prejudice %>% 
                            inner_join(get_sentiments("nrc") %>% 
                                         filter(sentiment %in% c("positive",
                                                                 "negative"))) %>% 
                            mutate(method = "NRC")) %>% 
  count(method, index = linenumber %/% 80, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
```

```{r}
bind_rows(afinn,
          bing_and_nrc) %>% 
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```

```{r}
get_sentiments("nrc") %>% 
                 filter(sentiment %in% c("positive", "negative")) %>%
  count(sentiment)

get_sentiments("bing") %>% 
  count(sentiment)

```

```{r}
bing_word_counts <- tidy_books %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(word, sentiment, sort = TRUE) %>% 
  ungroup()


bing_word_counts

bing_word_counts %>% 
  group_by(sentiment) %>% 
  top_n(10) %>% 
  ungroup() %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word,n,fill=sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```

```{r}
custom_stop_words <- bind_rows(tibble(word = c("miss"),
                                      lexicon = c("custom")),
                               stop_words)

custom_stop_words
```

```{r}
library(wordcloud)

tidy_books %>% 
  anti_join(stop_words) %>% 
  count(word) %>% 
  with(wordcloud(word, n, max.words = 100))
```

```{r}
library(reshape2)

tidy_books %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(word, sentiment, sort = TRUE) %>% 
  acast(word ~ sentiment, value.var = "n", fill = 0) %>% 
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)


```

```{r}
PandP_sentences <- tibble(text = prideprejudice) %>%
  unnest_tokens(sentence, text, token = "sentences")

PandP_sentences$sentence[2]
```

```{r}
austen_chapters <- austen_books() %>% 
  group_by(book) %>% 
  unnest_tokens(chapter, text, token = "regex",
                pattern = "Chapter|CHAPTER [\\dIVXLC]") %>% 
  ungroup()

austen_chapters %>% 
  group_by(book) %>% 
  summarise(chapters = n())

```

```{r}
#find the number of negative words in each chapter and divide by the total words in each chapter.
#for each book, which chapter has the highest proportion of negative words?

bingnegative <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")

wordcounts <- tidy_books %>% 
  group_by(book, chapter) %>% 
  summarize(words = n())

tidy_books %>% 
  semi_join(bingnegative) %>% 
  group_by(book, chapter) %>% 
  summarize(negativewords = n()) %>% 
  left_join(wordcounts, by = c("book", "chapter")) %>% 
  mutate(ratio = negativewords/words) %>% 
  filter(chapter != 0) %>% 
  top_n(1) %>% 
  ungroup()
```

#Chapter 3: Analyzing word and document frequency

```{r}
#The statistic tf-idf is intended to measure how important a word is to a document in a collection (or corpus) of documents, for example, to one novel in a collection of novels or to one website in a collection of websites. 

#idf(term) = ln(n_documents / n_documents containing term)

#term frequency in Jane Austen's novels

library(dplyr)
library(janeaustenr)
library(tidytext)

book_words <- austen_books() %>% 
  unnest_tokens(word, text) %>% 
  count(book, word, sort = TRUE)

total_words <- book_words %>% 
  group_by(book) %>% 
  summarize(total = sum(n))

book_words <- left_join(book_words, total_words)

book_words

#one row in this book_words df for each word-book combination; n is the number of times that word is used, and the total is total words in that books. The stop words are the usual suspects... let's viz

library(ggplot2)

ggplot(book_words, aes(n/total, fill = book)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~book, ncol = 2, scales = "free_y")


#these distributions are common in texts- the long tails are the rare words we are interested in.

```

```{r}
#Zipf's law states that the frequency that a word appears is inversely proportional to its rank.
#examine Zipf's law for Jane Austen's novels using dplyr

freq_by_rank <- book_words %>% 
  group_by(book) %>% 
  mutate(rank = row_number(),
         `term frequency` = n/total)

freq_by_rank

#rank tells us the rank of each word within the ferquency table; the table was already ordered by n, so we could use row_number() to find the rank. We can calculate the term frequency the same way we did before. Visualize Zipf's law by plotting rank on x-axis and term frequency on y-axis, using log scales. Plotting this way, an inversely proportional relationship will have a constant, negative slope.

freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = book)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()

#let's see the middle of the range

rank_subset <- freq_by_rank %>% 
  filter(rank < 500,
         rank > 10)

lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
#classic versions of Zipf's law have frequency (infinity) 1/rank - and we have in fact gotten a slope close to -1 (good). Let's plot this fitted power law with the data in Figure 3.3 to see how it looks

freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = book)) +
  geom_abline(intercept = -0.62, slope = -1.1, color = "gray50", linetype = 2) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
  scale_x_log10() +
  scale_y_log10()
```


```{r}
#the bind_tf_idf function

#the idea of tf-idf is to find the important words for the content of each document by decreasing the weight of commonly used words and increasing the weight for words that are not used very much in a collection (or "corpus") of documents - in this case, Jane Austen's novels as a whole. Calculating tf-idf attempts to find the words that are important (i.e. common) in a text, but not _too_ common. let's do that now.

#the bind_tf_idf function takes a tidy text dataset as input with one row per token (term) per document. One column (word here) contains the terms/tokens, one column contains the documents (book in this case), and the last necessary column contains the counts - how many times each document contains each term (n in this example). We calculated a total for each book from our explorations in previous sections, but it is not necessary for the bind_tf_idf function; the table only needs to contain all the words in each document.

book_words <- book_words %>% 
  bind_tf_idf(word, book, n)

book_words

#tf-idf is 0 for the most common words. let's arrange from the highest tf-idf (rarest words)

book_words %>% 
  select(-total) %>% 
  arrange(desc(tf_idf))

#now we see the proper nouns used the most - e.g. names such as elinor and marianne
#lets viz

book_words %>% 
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(book) %>% 
  top_n(15) %>% 
  ungroup() %>% 
  ggplot(aes(word, tf_idf, fill = book)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~book, ncol=2, scales = "free") +
  coord_flip()


```

```{r}
#a corpus of physics texts

#leaving fiction

library(gutenbergr)

physics <- gutenberg_download(c(37729, 14725, 13476, 30155),
                              meta_fields = "author")

#now that we have texts, use unnest_tokens() and count() to find out how many times each word was used in each text

physics_words <- physics %>% 
  unnest_tokens(word, text) %>% 
  count(author, word, sort = TRUE)

physics_words

#calc tf-idf and viz

library(forcats)

plot_physics <- physics_words %>% 
  bind_tf_idf(word, author, n) %>% 
  mutate(word = fct_reorder(word, tf_idf)) %>% 
  mutate(author = factor(author, levels = c("Galilei, Galileo",
                                            "Huygens, Christaan",
                                            "Tesla, Nikola",
                                            "Einstein, Albert")))


plot_physics %>% 
  group_by(author) %>% 
  top_n(15, tf_idf) %>% 
  ungroup() %>% 
  mutate(word = reodrer(word, tf_idf)) %>% 
  ggplot(aes(word, tf_idf, fill = author)) +
  geom_col(show.legened = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~author, ncol = 2, scales = "free") +
  coord_flip()

library(stringr)

physics %>% 
  filter(str_detect(text, "_k_")) %>% 
  select(text)

physics %>% 
  filter(str_detect(text, "RC")) %>% 
  select(text)

#cleaning of the text is in order. We have to come up with our own custom stop words to strip

mystopwords <- tibble(word = c("eq", "co", "rc", "ac", "ak", "bn",
                               "fig", "file", "cg", "cb", "cm",
                               "ab", "_k", "_k_", "_x"))

physics_words <- anti_join(physics_words, mystopwords, by = "word")

plot_physics <- physics_words %>% 
  bind_tf_idf(word, author, n) %>% 
  group_by(author) %>% 
  top_n(15, tf_idf) %>% 
  ungroup() %>% 
  mutate(word = reorder_within(word, tf_idf, author)) %>% 
  mutate(author = factor(author, levels = c("Galilei, Galileo",
                                            "Huygens, Christaan",
                                            "Tesla, Nikola",
                                            "Einstein, Albert")))

ggplot(plot_physics, aes(word, tf_idf, fill = author)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~author, ncol = 2, scales = "free") +
  coord_flip() +
  scale_x_reordered()


#one thing we can conclude is that we don't hear enough about ramparts or things being ethereal in physics today
  


```

```{r}
#4 Relationships between words - n-grams

#explore methods tidytext offers for calculating / viz relations between words in text dataset - including the token = "ngrams" argument - tokenizes by pairs of adjacent words rather than by individual ones.

#tokenizing by n-gram

#so far, we've been using unnest_tokens to tokenize by word, or sometimes by sentence, which is useful for the kinds of sentiment and frequency analyses we've done so far. But, we can also use the function to tokenize into consecutive sequences of words, caled __n-grams__. By seeing how often word X is followed by word Y, we can then build a model of the relationships between them.

#we do this by adding the token = "ngrams" option to unnest_tokens() and setting n to the number of words we wish to capture in each n-gram. When we set n to 2, we are examining pairs of two consecutie words, often called "bigrams"

library(dplyr)
library(tidytext)
library(janeaustenr)

austen_bigrams <- austen_books() %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

austen_bigrams


#counting and filtering n-grams

austen_bigrams %>% 
  count(bigram, sort = TRUE)

#use tidyr's separate() to split column into multiple based on a delimiter - separate into word1 and word2

library(tidyr)

bigrams_separated <- austen_bigrams %>% 
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word)

#new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigram_counts

#names are the most common pairs

#can use tidyr's unite() function to rejoin columns. separate/filter/count/unite lets us find the most common bigrams not containing stop-words

bigrams_united <- bigrams_filtered %>% 
  unite(bigram, word1, word2, sep = " ")

bigrams_united

#in other analyses, you may be interested in the most common trigrams; find by setting n = 3

austen_books() %>% 
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>% 
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word) %>% 
  count(word1, word2, word3, sort = TRUE)

#4.1.2 analyze bigrams
#one-bigram-per-row format is helpful for exploratory analyses of the text. E.g. we might be interested in the most common "streets" referenced in each book:

bigrams_filtered %>% 
  filter(word2 == "street") %>% 
  count(book, word1, sort = TRUE)

#a bigram can be treated as a term in a document the same way we treated individual words - e.g. look at tf-idf of bigrams across the corpus (austen novels). These tf-idf values can be visualized within each book, just as we did for words.

bigram_tf_idf <- bigrams_united %>% 
  count(book, bigram) %>% 
  bind_tf_idf(bigram, book, n) %>% 
  arrange(desc(tf_idf))

bigram_tf_idf

library(ggplot2)

bigram_tf_idf %>% 
  mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>% 
  group_by(book) %>% 
  top_n(15) %>% 
  ungroup() %>% 
  ggplot(aes(bigram, tf_idf, fill = book)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~book, ncol=2, scales = "free") +
  coord_flip()

#4.1.3 using bigrams to provide context in  sentiment analysis
#Chapter 2 approach simply counted appearance of positive or negative words, according to a reference lexicon. One of the problems here is that a word's context can matter nearly as much as its presence - e.g. "happy" and "like" will be counted as positive, even in a sentence like "I'm not _happy_ and I don't _like_ it!"

#now that we have data organized into bigrams, it's easy to tell how often words are preceded by a word like "not."

bigrams_separated %>% 
  filter(word1=="not") %>% 
  count(word1, word2, sort = TRUE)

#use the "not" to reverse sentiment score. first use AFINN to get sentiments

AFINN <- get_sentiments("afinn")

#then examine most frequent words preceded by "not" AND were associated with a sentiment

not_words <- bigrams_separated %>% 
  filter(word1 == "not") %>% 
  inner_join(AFINN, by = c(word2 = "word")) %>% 
  count(word2, value, sort = TRUE)

not_words

#which words contributed the most in the "wrong" direction? multiply their value by the number of times they appear. Viz it:

not_words %>% 
  mutate(contribution = n * value) %>% 
  arrange(desc(abs(contribution))) %>% 
  head(20) %>% 
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, n * value, fill = n* value > 0)) +
  geom_col(show.legend = FALSE) +
  xlab("Words preceded by \"not\"") +
  ylab("Sentiment value * number of occurences") +
  coord_flip()

#the bigrams "not like" and "not help" are the largest causes of misidentification, making the text seem much more positive than it is. But we can see phrases like "not afraid" and "not fail" sometimes suggest text is more negative than it is.

#other terms doing this negation of subsequent term - e.g. "no", "never", "without" - run these at the same time:
negation_words <- c("not", "no", "never", "without")

negated_words <- bigrams_separated %>% 
  filter(word1 %in% negation_words) %>%
  inner_join(AFINN, by = c(word2 = "word")) %>% 
  count(word1, word2, value, sort = TRUE) %>% 
  mutate(contribution = n * value) %>% 
  arrange(desc(abs(contribution))) %>% 
  head(100) %>% 
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, n * value, fill = n* value > 0)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~word1, ncol=2, nrow = 2, scales="free")+
  coord_flip()

negated_words
  #xlab("Words preceded by \"negation words\"") +
  #ylab("Sentiment value * number of occurences") +
  

#you'll have to do more research on your own as far as how to reverse the sentiment scores for these negate cases


  
```

```{r}
#4.1.4 viz network of bigrams with ggraph

#we may be interested in viz all relationships among words simultaneously, rather than just a top few at a time.
  # one option is a network graph
  # this can be constructed from a tidy object as it has three vars:
    #1 from: the node an edge is coming from
    #2 to: the node an edge is going towards
    #3 weight: a numeric value associated with each edge

#use igraph to manipulate and analyze networks. 
  #one option is graph_from_data_frame() fn, which takes a df of edges with columns for "from","to",and edge attributes ("n") in this case

library(igraph)

#original counts
  bigram_counts
  
  
  # filter for only relatively common combinations
  
bigram_graph <- bigram_counts %>% 
  filter(n > 20) %>%
  graph_from_data_frame()

bigram_graph

#igraph not designed for plotting, so we have to convert igraph object into ggraph (as one of the options)
  #have to add layers - need at least nodes, edges and text

library(ggraph)
set.seed(2017)

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link()+
  geom_node_point()+
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)

#the node graph allows viz some details of text structure; cluster of things following salutations, as well as shorter pairings that appear frequently

#let's add polish to the graph
# - edge_alpha to link layer to make links transparent based on how common or rare the bigram is
# - add directionality with an arrow, using grid::arrow(), including end_cap option that tells the arrow to end before touching the node
# - tinker with options to the node layer to make the nodes more attractive (larger, blue points)
# we add a theme that's useful for plotting networks - theme_void

set.seed(2016)

a <- grid::arrow(type = "closed", length = unit(0.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches'))+
  geom_node_point(color = "lightblue", size=5)+
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()

```




















